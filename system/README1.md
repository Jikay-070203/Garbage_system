ğŸ—‘ï¸ Waste Classification Triton API  
FastAPI-based inference server for waste classification using YOLO models converted to ONNX and deployed via NVIDIA Triton Inference Server. Supports both CPU and GPU runtime. Designed for scalable, production-ready deployment (Docker).

ğŸš€ **Features**  
ğŸ” Real-time waste classification using YOLO.onnx  
âš¡ Fast inference with ONNX models + Triton Server  
ğŸ“¡ API-ready with FastAPI + Triton gRPC/HTTP  
â˜ï¸ Cloud-ready: Docker, Compose, Kubernetes, Helm  
ğŸ“¤ API Endpoint: `POST /predict`

ğŸ—ï¸ **Project Structure**

```
.
â”œâ”€â”€ app/                    # FastAPI app source code
â”‚   â”œâ”€â”€ app.py              # API Endpoint
â”‚   â””â”€â”€ triton_clients/     # Triton gRPC clients for YOLO ONNX model
â”œâ”€â”€ models/                 # ONNX model repository for Triton
â”œâ”€â”€ Dockerfile              # Docker build config
â”œâ”€â”€ docker-compose.yml      # Multi-container deployment (CPU/GPU)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ triton_clients/     # Triton gRPC or HTTP clients for model inference
â”‚   â”‚   â”œâ”€â”€ yolo_client.py          # YOLO model inference client
â”‚   â”‚   â”œâ”€â”€ vae_encoder_client.py   # VAE Encoder client
â”‚   â”‚   â”œâ”€â”€ vae_decoder_client.py   # VAE Decoder client
â”‚   â”‚   â”œâ”€â”€ unet_client.py          # UNet client
â”œâ”€â”€ charts/                 # Helm chart for Kubernetes deployment
â”‚   â””â”€â”€ waste-classifier/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ values.yaml
â”‚       â””â”€â”€ templates/
â”‚           â”œâ”€â”€ deployment.yaml
â”‚           â””â”€â”€ service.yaml
â”œâ”€â”€ k8s/                    # Kubernetes manifests
â”‚   â”œâ”€â”€ waste-classifier-deploy.yaml
â”‚   â””â”€â”€ waste-classifier-service.yaml
â””â”€â”€ README.md
```

ğŸ³ **Setup with Docker**

```bash
docker run --gpus=all --rm -p8000:8000 -p8001:8001 -p8002:8002 \
    -v /path/to/model:/models nvcr.io/nvidia/tritonserver:23.10-py3 \
    tritonserver --model-repository=/models
```

ğŸš€ **Run FastAPI Server**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload
```

ğŸ”¨ **Build & Run with Docker**

```bash
docker build -t waste-classifier-triton .
docker run --gpus all -p 8000:8000 waste-classifier-triton
```
